# 📘 Chapter 05 – Multilayer Perceptrons

<!-- TODO write summary -->
no notebook for 5.3 because there was nothing to code there

---

## 📂 Contents

| Notebook | Topic |
|----------|-------|
| `05_01_multilayer_perceptrons` | Multilayer Perceptrons, nonlinear activation functions |
| `05_02_implementation_of_multilayer_perceptrons` | MLP implementation |
| `05_04_numerical_stability_and_initialization` | vanishing & exploding gradients, parameter initialization |

---

## 📌 Topics Covered by Subchapter

### 5.1. Multilayer Perceptrons

Building Multilayer Perceptrons by introducing hidden layers and adding more capabilities than with a single layer by introducing nonlinear activation functions (ReLu, sigmoid, tanh).

### 5.2. Implementation of Multilayer Perceptrons

Implementation of MLP with one hidden layer both from scratch and using high level Pytorch functions

### 5.3. Forward Propagation, Backward Propagation, and Computational Graphs

A deeper look into forward propagation, backward propagation and computational graphs as means for visualization.

### 5.4. Numerical Stability and Initialization

Stating the problems of vanishing and exploding gradients and parameter initialization strategies to account for that.

---

➡️ Next up: [Chapter 06 – Builders’ Guide](../chapter_06_builders_guide/)
