# üìò Chapter 02 ‚Äì Preliminaries

<!--TODO: write text-->
<!-- This chapter covers essential mathematical and programming foundations needed for deep learning. I implemented the concepts in separate notebooks, each focusing on one topic.-->

---

## üìÇ Contents

| Notebook | Topic |
|----------|-------|
| `02_01_data_manipulation.ipynb` | Tensor basics in PyTorch |
| `02_02_data_preprocessing.ipynb` | Introduction to Pandas |
| `02_03_linear_algebra.ipynb` | Vectors, matrices, tensors and their operations |
| `02_04_calculus.ipynb` | derivatives, differentiation, partial derivatives, chain rule |
| `02_05_automatic_differentiation` | grad() and backward() Pytorch functions |

---

## üìå Topics Covered by Subchapter

### 2.1. Data Manipulation

Learned basic tensor operations using PyTorch‚Äôs `torch.tensor` and `torch.arange`. This included indexing, slicing, broadcasting, saving memory through in place operations and conversions of tensors to other Python objects.

### 2.2. Data Preprocessing

Explored data loading with pandas, data preperation and conversion to the tensor format.

### 2.3. Linear Algebra

Revised linear algebra basics and their Pytorch implementations. This included scalars, vectors, matrices, tensors, reduction, dot product and matrix multiplication.

### 2.4. Calculus

Revised calculus basics and visualization methods. This included derivatives, differentiation, partial derivatives and the chain rule.

### 2.5. Automatic Differentiation

Introduction to the mechanics of the grad() and backward() Pytorch functions as well as detaching.

---

‚û°Ô∏è Next up: [Chapter 03 ‚Äì Linear Neural Networks for Regression](../chapter_03_linear_neural_networks_for_regression/)
